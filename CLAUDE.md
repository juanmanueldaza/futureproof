# FutureProof

Career intelligence system: AI agent gathers professional data (LinkedIn, portfolio, CliftonStrengths), indexes to ChromaDB, analyzes career trajectories, and generates CVs. GitHub/GitLab data accessed live. Everything through a chat interface.

## Commands

```bash
futureproof chat                    # Interactive chat
futureproof chat --debug            # With debug logs
futureproof chat --thread work      # Named thread
futureproof ask "question"          # One-off question
futureproof memory --threads        # List threads
futureproof memory --clear          # Clear history

pytest tests/ -q                    # Unit tests
pytest tests/eval/ -m eval          # Eval tests (need Azure creds)
pyright src/futureproof             # Type check
ruff check .                        # Lint
ruff check . --fix                  # Auto-fix
```

## Project Structure

```
src/futureproof/
├── agents/
│   ├── career_agent.py     # create_agent(), singleton, 4 middlewares
│   ├── middleware.py        # dynamic_prompt, repair, synthesis, summarization
│   ├── orchestrator.py      # LangGraph Functional API for analysis workflows
│   ├── state.py             # TypedDict state definitions
│   ├── helpers/             # Orchestrator support (data_pipeline, llm_invoker, result_mapper)
│   └── tools/               # 39 tools: profile, gathering, github, gitlab, knowledge,
│                            #   analysis, generation, market, financial, memory
├── chat/                    # Streaming client, HITL loop, Rich UI
├── gatherers/               # LinkedIn CSV, CliftonStrengths PDF, portfolio scraper, market data
├── generators/              # CV generation (Markdown + PDF via WeasyPrint)
├── llm/                     # FallbackLLMManager, 5-model Azure chain, purpose routing
├── memory/                  # ChromaDB stores (knowledge + episodic), chunker, profile, embeddings
├── mcp/                     # 12 clients: GitHub, Tavily, job boards, HN, financial, content
├── prompts/                 # System + analysis + CV prompt templates (markdown files)
├── services/                # GathererService, AnalysisService, KnowledgeService
└── utils/                   # PII anonymization, data loading, logging
```

## Architecture

- **Single agent** with `create_agent()` — all 39 tools, no multi-agent handoffs
- **4 middlewares** (in order): `build_dynamic_prompt` (injects live profile + knowledge stats), `ToolCallRepairMiddleware` (fixes orphaned tool_calls after HITL), `AnalysisSynthesisMiddleware` (two-pass: masks tool results, replaces generic response with focused synthesis), `SummarizationMiddleware` (32k token trigger, cheaper model)
- **Data flow**: Gatherers return `list[Section]` → `index_sections()` → ChromaDB → search/retrieval via `KnowledgeService`
- **HITL**: `interrupt()` on `generate_cv`, `gather_all_career_data`, `clear_career_knowledge`
- **LLM**: Azure OpenAI only. `FallbackLLMManager` with 5-model chain (GPT-4.1 → GPT-5 Mini → GPT-4o → GPT-4.1 Mini → GPT-4o Mini). Purpose-based routing: `agent`, `analysis`, `summary`, `synthesis` — each can use a different deployment
- **Orchestrator**: LangGraph Functional API (`@entrypoint`/`@task`) for analysis workflows
- **Memory**: ChromaDB for career knowledge RAG + episodic memory (decisions, applications). SQLite checkpointer for conversations.

## Code Conventions

- Python 3.13 with type hints. Use `collections.abc` (`Mapping`, `Sequence`) not `typing`.
- Lines under 100 characters
- Raise exceptions, don't return error dicts. Use `ServiceError`/`NoDataError`/`AnalysisError` hierarchy.
- Dependency injection for testability
- Sync tool functions with `run_async()` helper for async service calls
- `KnowledgeSource` is an enum — convert string → `KnowledgeSource(source)` in tools

## Git Commits

- NEVER add `Co-Authored-By`, `Generated by`, or any AI attribution
- NEVER claim authorship or credit in code, comments, or commit messages
- Conventional commits: concise subject, optional body with technical details

## Testing

- Mock external services (LLM, HTTP, ChromaDB) — no real API calls in unit tests
- Eval tests (`tests/eval/`) require Azure credentials and use `@pytest.mark.eval`
- Fixtures in `conftest.py` for common test data

## Security

- PII anonymization (`utils/security.py`) before sending career data to LLMs
- SSRF protection in portfolio fetcher (blocks private IPs)
- `subprocess.run()` with list args, no `shell=True`, 30s timeout
- Sensitive files (CVs, profile) created with `0o600` permissions
